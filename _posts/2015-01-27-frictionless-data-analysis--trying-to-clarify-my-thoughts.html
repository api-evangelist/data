---
layout: post
title: 'Frictionless Data Analysis – Trying to Clarify My Thoughts'
url: http://blog.ouseful.info/2015/01/27/frictionless-data-analysis-trying-to-clarify-my-thoughts/
source: http://blog.ouseful.info/2015/01/27/frictionless-data-analysis-trying-to-clarify-my-thoughts/
domain: blog.ouseful.info
image: http://kinlane-productions.s3.amazonaws.com/screen-capture-api/blog-ouseful-info20150127frictionless-data-analysis-trying-to-clarify-my-thoughts.png
---

<p>Prompted by a conversation with Rufus Pollock over lunch today, in part about data containerisation and the notion of “frictionless” data that can be easily discovered and is packaged along with metadata that helps you to import it into other tools or applications (such as a database), I’ve been confusing myself about what it might be like to have a frictionless data analysis working environment, where I could do something like write fda --datapackage http://example.com/DATAPACKAGE --db postgres --client rstudio ipynb and that would then:
The idea is that from a single command I can pull down a datafile, ingest it into a database, fire up one or more clients that are connected to that database, and start working with the data immediately.It’s not so different to double clicking on a file on your desktop and launching it into an application to start working on it, right?Can’t be that hard to wire up, surely?;-) But would it be useful?</p>
